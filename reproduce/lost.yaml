# Example configuration for LOST-inspired factorization
model_path: "meta-llama/Meta-Llama-3-8B-Instruct" 
dataset: "Open-Orca/SlimOrca"
dataset_column: "text"
dataset_subset: "train"
dataset_size: 2000
batch_size: 4
max_length: 512

# Method selection
method: "lost_inspired"  # New LOST-inspired method

# Block selection parameters
layers_to_skip: 4  # For 4-block replacement
num_A: 1
merge_consecutive: False
distances_path: null

# OPTIMIZED LOST-inspired parameters
low_rank: 1024  # Rank for low-rank component
sparsity_ratio: 0.05  # INCREASED from 0.01 to 0.05 (5% channels)
gamma: 0.3  # DECREASED from 0.7 to 0.3 (better balance, less aggressive)
loss: "cosine"
solver: "adam"

# Other parameters
use_4bit: True
save_transform_only: True
diag: False
thri: False
two_vectors: False
accurate: False
