method: "hyper"
model_path: "meta-llama/Llama-3.2-3B-Instruct"
dataset: "Open-Orca/SlimOrca"
dataset_column: "text"
batch_size: 4
max_length: 512
layers_to_skip: 4
dataset_size: 1000
dataset_subset: "train"
use_4bit: False
save_path: null
token: null  # Add your HuggingFace token if needed

# HyperReplace specific parameters
hyper_rank: 64  # Rank for low-rank decomposition
hyper_lr: 0.00005  # Learning rate for HyperNetwork
hyper_epochs: 20  # Number of training epochs
adaptive_selection: True  # Enable adaptive block selection
complexity_threshold: [0.3, 0.7]  # Complexity thresholds
progressive_stages: True  # Use progressive training

# Block selection parameters
distances_path: null  # Will be computed if not provided
num_A: 1  # Number of blocks to replace
merge_consecutive: False

# Evaluation tasks
tasks:
  boolq:
    fewshots: 0
  lambada_openai:
    fewshots: 0