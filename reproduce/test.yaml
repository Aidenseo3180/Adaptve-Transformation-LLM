method: "laco"
model_path: "meta-llama/Llama-3.2-3B-Instruct"
dataset: "Open-Orca/SlimOrca"
dataset_column: "text"
batch_size: 4
max_length: 512
layers_to_skip: 4
dataset_size: 1000
dataset_subset: "train"
use_4bit: False  # CPU에서 weight 추출
save_path: null

distances_path: null
num_A: 1
merge_consecutive: False

# Layer Collapse specific
preserve_ratio: 0.1       # 차이의 10%만 보존
fine_tune_epochs: 5       # Fine-tuning epochs
learning_rate: 0.00001    # Fine-tuning LR

tasks:
  boolq:
    fewshots: 0
  lambada_openai:
    fewshots: 0