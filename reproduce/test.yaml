method: "optimized_plds"
model_path: "meta-llama/Llama-3.2-3B-Instruct"
dataset: "Open-Orca/SlimOrca"
dataset_column: "text"
batch_size: 4
max_length: 512
layers_to_skip: 4
dataset_size: 1000
dataset_subset: "train"
use_4bit: False
save_path: null
min_distance_layer: 20

# Standard parameters
distances_path: "distances.pth"
num_A: 1
merge_consecutive: False

# Optimized PLDS specific
use_correction: True           # Use correction network
correction_dim_reduction: 16   # Hidden dim reduction factor
distillation_epochs: 15        # Training epochs
learning_rate: 0.00005         # Base learning rate
warmup_epochs: 2               # Warmup epochs
alpha_mse: 0.7                 # MSE loss weight
alpha_cos: 0.2                 # Cosine loss weight
alpha_intermediate: 0.1        # Intermediate supervision weight
use_pretrained_init: True      # Initialize with ReplaceMe transform

# Evaluation
tasks:
  boolq:
    fewshots: 0
  lambada_openai:
    fewshots: 0