method: "dtb"
model_path: "meta-llama/Llama-3.2-3B-Instruct"
dataset: "Open-Orca/SlimOrca"
dataset_column: "text"
batch_size: 4
max_length: 512
layers_to_skip: 4
dataset_size: 1000
dataset_subset: "train"
use_4bit: False
save_path: null
min_distance_layer: 20
distances_path: "distances.pth"

# DTB specific parameters
min_rank: 4
max_rank: 64
use_attention_delta: False  # Use attention-based delta for high-change layers
num_A: 1
merge_consecutive: True

# Evaluation tasks
tasks:
  boolq:
    fewshots: 0
  lambada_openai:
    fewshots: 0
  winogrande:
    fewshots: 5
  piqa:
    fewshots: 0