method: "kd_guided"
model_path: "meta-llama/Meta-Llama-3-8B-Instruct"
dataset: "Open-Orca/SlimOrca"
dataset_column: "text"
batch_size: 8
max_length: 1024
layers_to_skip: 8
dataset_size: 8000  # Will use min(this, 50000) for teacher outputs
dataset_subset: "train"
use_4bit: True
save_path: null
distances_path: "distances_8layer.pth"
num_A: 1
merge_consecutive: False

# Knowledge Distillation specific parameters
kd_epochs: 10  # Number of epochs for T optimization
kd_lr: 0.0001  # Learning rate for T optimization  
kd_alpha: 0.8  # Weight for intermediate loss (vs cosine loss)