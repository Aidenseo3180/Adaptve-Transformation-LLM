method: "residual_linear"
model_path: "meta-llama/Meta-Llama-3-8B-Instruct"
dataset: "Open-Orca/SlimOrca"
dataset_column: "text"
batch_size: 4
max_length: 512
layers_to_skip: 4
dataset_size: 2000
dataset_subset: "train"
use_4bit: False
save_path: null
min_distance_layer: 20
save_transform_only: True
solver: "adam"
loss: "cosine" # cosine, mse are supported
distances_path: null
num_A: 1
merge_consecutive: False 

# Residual Linear Approximation specific parameters
num_residual_layers: 3  # Number of residual transformation layers to learn
# Layer 1: Basic transformation
# Layer 2: Attention-aware refinement  
# Layer 3: Context-specific adjustments

# Evaluation settings (optional)
