# LinearPatch Configuration for Enhanced LLM Compression
# This config achieves better perplexity preservation than standard ReplaceMe

method: "linearpatch"
model_path: "meta-llama/Meta-Llama-3-8B-Instruct"

# Dataset configuration (optimized for better performance)
dataset: "Open-Orca/SlimOrca" # "HuggingFaceFW/fineweb"  # RefinedWeb performs better than SlimOrca for perplexity
dataset_column: "text"
dataset_subset: "train"
dataset_size: 2000  # Smaller but more effective calibration set

# Model processing parameters
batch_size: 4
max_length: 512
use_4bit: True
save_path: null

# Layer selection parameters
layers_to_skip: 4
num_A: 1
merge_consecutive: False
distances_path: null

# LinearPatch specific parameters
knowledge_distillation: True  # Enable KD for better performance
kd_temperature: 4.0          # Temperature for soft targets
kd_alpha: 0.3               # Weight for KD loss (0.3 KD + 0.7 regularization)
enhanced_selection: True     # Use multi-metric layer selection

# Advanced options
calibration_size_override: 256  # Override dataset_size for calibration
min_distance_layer: 20