# Example configuration for residual low-rank factorization
model_path: "meta-llama/Meta-Llama-3-8B-Instruct"  # Replace with your 3B model
dataset: "Open-Orca/SlimOrca"
dataset_column: "text"
dataset_subset: "train"
dataset_size: 2000
batch_size: 4
max_length: 512

# Method selection
method: "residual_low_rank"  # New residual method

# Block selection parameters
layers_to_skip: 4  # For 4-block replacement
num_A: 1
merge_consecutive: False
distances_path: null

# Residual low-rank specific parameters
low_rank: 1024  # Rank for factorization
alpha_candidates: [0.1, 0.2, 0.3, 0.4, 0.5]  # Alpha values to test
auto_tune_alpha: True  # Enable automatic alpha tuning
loss: "cosine"
solver: "adam"

# Other parameters
use_4bit: True
save_transform_only: True
diag: False
thri: False
two_vectors: False
accurate: False

# # Evaluation tasks
# tasks:
#   boolq:
#     fewshots: 0
#   lambada_openai:
#     fewshots: 0
#   openbookqa:
#     fewshots: 0
#   piqa:
#     fewshots: 0
#   race:
#     fewshots: 0
#   sciq:
#     fewshots: 0
#   winogrande:
#     fewshots: 5